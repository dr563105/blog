{
  
    
        "post0": {
            "title": "Getting started with S3 using boto3",
            "content": "Boto3 is an AWS python SDK that allows access to AWS services like EC2 and S3. It provides a python object-oriented API and as well as low-level access to AWS services . !pip install boto3 . import boto3, botocore import glob . files = glob.glob(&#39;data/*&#39;) #to upload multiple files files . [&#39;data/Player Data.xlsx&#39;, &#39;data/30-days-create-folds.ipynb&#39;, &#39;data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv&#39;, &#39;data/star_pattern_turtlesim.png&#39;] . Create a session and client . Boto3&#39;s region defaults to N-Virginia. To create buckets in another region, region name has to be explicitly mentioned using session object. . session = boto3.Session(region_name=&#39;us-east-2&#39;) s3client = session.client(&#39;s3&#39;) s3resource = boto3.resource(&#39;s3&#39;) . S3 buckets have to follow bucket naming rules. . bucket_names = [&#39;my-s3bucket1-usohio-region&#39;, &#39;my-s3bucket2-usohio-region&#39;] s3location = {&#39;LocationConstraint&#39;: &#39;us-east-2&#39;} . Check if bucket exists in S3 . Checking for something before creation is one of the important tasks to avoid unnecessary errors. Here we check if the buckets already exists. . def check_bucket(bucket): &quot;&quot;&quot; Checks if a bucket is present in S3 args: bucket: takes bucket name &quot;&quot;&quot; try: s3client.head_bucket(Bucket=bucket) print(&#39;Bucket exists&#39;) return True except botocore.exceptions.ClientError as e: # If a client error is thrown, then check that it was a 404 error. # If it was a 404 error, then the bucket does not exist. error_code = int(e.response[&#39;Error&#39;][&#39;Code&#39;]) if error_code == 403: print(&quot;Private Bucket. Forbidden Access!&quot;) return True elif error_code == 404: print(&quot;Bucket Does Not Exist!&quot;) return False . for bucket in bucket_names: print(check_bucket(bucket)) . Bucket Does Not Exist! False Bucket Does Not Exist! False . Create a bucket in S3 . If the buckets don&#39;t exist, we create them. We need to supply bucket name, a dictionary specifying in which region the bucket has to be created. . for bucket_name in bucket_names: if not(check_bucket(bucket_name)): print(&#39;Creating a bucket..&#39;) s3client.create_bucket(Bucket = bucket_name, CreateBucketConfiguration=s3location) . Bucket Does Not Exist! Creating a bucket.. Bucket Does Not Exist! Creating a bucket.. . Bucket Versioning . Bucket versioning initial state is not set by default. The response from when not initialised doesn&#39;t carry status information rather status dict is absent. Status expects two return states: enabled, suspended. On first creation, the status is in disabled, an unknown state. . So in order to make it appear in the REST response, bucket must be enabled by calling the BucketVersioning() boto3 resource function. If we then check the status, it will be present in the REST response. . def get_buckets_versioning_client(bucketname): &quot;&quot;&quot; Checks if bucket versioning is enabled/suspended or initialised Args: bucketname: bucket name to check versioning Returns: response status - enabled or suspended &quot;&quot;&quot; response = s3client.get_bucket_versioning(Bucket = bucketname) if &#39;Status&#39; in response and (response[&#39;Status&#39;] == &#39;Enabled&#39; or response[&#39;Status&#39;] == &#39;Suspended&#39;): print(f&#39;Bucket {bucketname} status: {response[&quot;Status&quot;]}&#39;) return response[&#39;Status&#39;] else: print(f&#39;Bucket versioning not initialised for bucket: {bucketname}. Enabling...&#39;) s3resource.BucketVersioning(bucket_name=bucketname).enable() enable_response = s3resource.BucketVersioning(bucket_name=bucket_name).status return enable_response . for bucket_name in bucket_names: version_status = get_buckets_versioning_client(bucket_name) print(f&#39;Versioning status: {version_status}&#39;) . Bucket versioning not initialised for bucket: my-s3bucket1-usohio-region. Enabling... Versioning status: Enabled Bucket versioning not initialised for bucket: my-s3bucket2-usohio-region. Enabling... Versioning status: Enabled . To suspend bucket versioning . for bucket_name in bucket_names: version_status = get_buckets_versioning_client(bucket_name) print(f&#39;Versioning status: {version_status}&#39;) if version_status == &#39;Enabled&#39;: print(&#39;Disabling again..&#39;) s3resource.BucketVersioning(bucket_name=bucket_name).suspend() . Bucket my-s3bucket1-usohio-region status: Enabled Versioning status: Enabled Disabling again.. Bucket my-s3bucket2-usohio-region status: Enabled Versioning status: Enabled Disabling again.. . To enable bucket versioning . for bucket_name in bucket_names: version_status = get_buckets_versioning_client(bucket_name) print(f&#39;Versioning status: {version_status}&#39;) if version_status == &#39;Suspended&#39;: print(&#39;Enabling again..&#39;) s3resource.BucketVersioning(bucket_name=bucket_name).enable() . Bucket my-s3bucket1-usohio-region status: Suspended Versioning status: Suspended Enabling again.. Bucket my-s3bucket2-usohio-region status: Suspended Versioning status: Suspended Enabling again.. . Get bucket list from S3 . We can list the buckets in S3 using list_buckets() client function. It return a dict. We can iterate through Buckets key to find the names of the buckets. . buckets_list = s3client.list_buckets() for bucket in buckets_list[&#39;Buckets&#39;]: print(bucket[&#39;Name&#39;]) . my-s3bucket1-usohio-region my-s3bucket2-usohio-region . Upload files to S3 . Boto3 allows file upload to S3. The upload_file client function requires three mandatory arguments - . 1. filename of the file to be uploaded 2. bucket_name, Into which bucket the file would be uploaded 3. key, name of the file in S3 . def upload_files_to_s3(filename, bucket_name, key=None, ExtraArgs=None): &quot;&quot;&quot; Uploads file to S3 bucket Args: filename: takes local filename to be uploaded bucker_name: name of the bucket into which the file is uploaded key: name of the file in the bucket. Default:None ExtraArgs: other arguments. Default:None &quot;&quot;&quot; if key is None: key = filename try: s3client.upload_file(filename,bucket_name,key) print(f&#39;uploaded file:{filename}&#39;) except botocore.exceptions.ClientError as e: print(e) . We can make use of glob module to upload multiple files in a folder . bucket1_files = [files[1],files[2]] bucket2_files = [files[0],files[3]] bucket1_files, bucket2_files . ([&#39;data/30-days-create-folds.ipynb&#39;, &#39;data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv&#39;], [&#39;data/Player Data.xlsx&#39;, &#39;data/star_pattern_turtlesim.png&#39;]) . for file in bucket1_files: upload_files_to_s3(file,bucket_name=bucket_names[0]) . uploaded file:data/30-days-create-folds.ipynb uploaded file:data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv . for file in bucket2_files: upload_files_to_s3(file,bucket_name=bucket_names[1]) . uploaded file:data/Player Data.xlsx uploaded file:data/star_pattern_turtlesim.png . Get files list . Getting the files list from each bucket done using list_objects client function. It returns dict and we can iterate through Contents key to retrieve the filenames. . for bucket in bucket_names: print(f&#39;Listing object inside bucket:{bucket}&#39;) list_obj_response = s3client.list_objects(Bucket=bucket) for obj in list_obj_response[&#39;Contents&#39;]: print(obj[&#39;Key&#39;]) print() . Listing object inside bucket:my-s3bucket1-usohio-region data/30-days-create-folds.ipynb data/ARK_GENOMIC_REVOLUTION_ETF_ARKG_HOLDINGS.csv Listing object inside bucket:my-s3bucket2-usohio-region data/Player Data.xlsx data/star_pattern_turtlesim.png . Download files . Downloading a file is very similar to uploading one. We need specify bucket name, name of the file to be downloaded, and the destination filename. . print(f&#39;Downloading files from bucket:{bucket_names[1]}&#39;) s3client.download_file(Bucket=bucket_names[1],Key=&#39;data/star_pattern_turtlesim.png&#39;,Filename=&#39;downloaded_turtlesim.jpg&#39;) . Downloading files from bucket:my-s3bucket2-usohio-region . Conclusion . This blog post shows how to use the boto3 python SDK to manage S3 aws service. With the help of documentation, we can implement require functionalities. .",
            "url": "https://dr563105.github.io/blog/s3_with_boto3/",
            "relUrl": "/s3_with_boto3/",
            "date": " • Apr 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Setup Skim PDF reader with VimTeX in Mac OS",
            "content": "VimTeX plugin written by Karl Yngve Lervåg is one of the goto plugins to manage LaTeX files with Vim/Neovim text editors. VimTeX allows integration with several PDF viewers. In Mac OS, Skim and Zathura PDF readers allows easy integration with LaTeX. Since Zathura’s installation in Mac OS involves more steps, we will be using Skim for this post. . . You should have a working LaTeX setup Install Skim . With Homebrew . brew install --cask skim . Or download the dmg file of the current version(as of writing latest version is v1.6.8) from Skim’s website. . Install VimTeX . Using vim-plug plugin manager we add the following line to .vimrc or init.vim or init.lua . Plug &#39;lervag/vimtex&#39; . Pdf preview . Conversion between TeX and PDF is one of the most common operations while writing a scientific document. Though it is possible to open the PDF file in one of the commercially available PDF readers, a seamless integration with neovim(in our case) is appreciated. This is where Skim comes into the picture. By default, Skim allows native, seamless integration with the LaTex editor of choice. In our case, we can make VimTex interact with Skim with just a few lines of config. . Configurations . Minimal setup and Forward Search . We require the following lines to make VimTeX talk to Skim within neovim. This direction of communication, is known as forward search. . let g:tex_flavor=&#39;latex&#39; # Default tex file format let g:vimtex_view_method = &#39;skim&#39; # Choose which program to use to view PDF file let g:vimtex_view_skim_sync = 1 # Value 1 allows forward search after every successful compilation let g:vimtex_view_skim_activate = 1 # Value 1 allows change focus to skim after command `:VimtexView` is given . The forward search allows any change made in the TeX file automatically refreshes Skim to reflect those changes in PDF. One of the other common uses is cursor sync between the TeX file and PDF. Setting let g:vimtex_view_skim_sync allows placing the cursor in some position in the Tex file sync with the same position in the PDF after every successful compilation(:VimtexCompile). Setting let g:vimtex_view_skim_activate allows to shift focus of control from neovim to Skim and bring it to foreground. . Inverse or Backward Search . So far there was only one channel of communication between neovim(editor) and Skim. A backward communication is possible but it took quite bit of hacking to get it to work. More on that read this jdhao’s post. However, with the release of VimTex v2.8, it has become simple to setup. . Consider a scenario where we are going through a paper and find an error, instead of going back to source TeX file and finding the error location can be cumbersome. Using backward search, we can go to the error location from PDF to TeX. For Skim, to activate backward search press shift and command together and click the position in PDF using the mouse. That location gets reflected in the editor in the background. For more information, see :h :VimtexInverseSearch . Natively, every instance of neovim starts a server 1. With Skim as client and nvim as server, we can interact in that direction. . In order to do so, in the preferances pane of Skim, navigate to Sync tab. There, in the PDF-TeX Sync support, make preset as custom, command as nvim(use vim if you use vim editor), and set arguments as --headless -c &quot;VimtexInverseSearch %line &#39;%file&#39;&quot;. . Important: Skim must be started by VimTeX (either through compiler callback or explicitly via lv) for backward sync to work! (This is how Skim “knows” which neovim instance – terminal or GUI – to sync to.) . Conclusion . With just four lines of settings in the init.vim file and a line in Skim preferances, we can activate both forward and backward search features with VimTeX. . Footnotes . In the nvim command line, run :echo v:servername to know the name of the server &#8617; . |",
            "url": "https://dr563105.github.io/blog/skim-vimtex-setup/",
            "relUrl": "/skim-vimtex-setup/",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Manage dotfiles with GNU Stow",
            "content": "In this post, I will try to guide in organise your dotfiles in the cloud and manage them using GNU Stow. . What are dotfiles? . For a casual user, the term dotfiles may sound strange and confusing but it is nothing but application(app) configuration files in developer talk. The apps generally refer to certain files to configure itself. . People usually store these files in a remote location such as a Github repository and retrieve them when needed. . Dotfiles allow personalisation. They can be restored in a new machine saving time. Preparing and organising the dotfiles with some initial effort, help developers save a lot of time later. . A few examples of dotfiles are .bashrc, .vimrc, .gitignore. . . Warning: Pay attention to personal information inside these files. Never store secure keys, passwords in public domains. Things to know . Which app’s config files need to stored. | Where do those config files are located. | . Common config files that need storing . .bashrc or .zshrc | .vimrc or init.vim(in the case of neovim) | .gitignore_global and .gitconfig i | Terminal emulator config files | IDE of choice config files | . In fact, if there is an app that you have configured heavily and frequently use, its config files must be stored. In the case the said app doesn’t allow exporting of configurations, it is highly recommended to move onto one that allows it. . Where are most required dotfiles located? . Most files are present in $HOME or $XDG_CONFIG_HOME directories. $XDG_CONFIG_HOME defines the base directory relative to which user-specific configuration files should be stored. If $XDG_CONFIG_HOME is either not set or empty, a default equal to $HOME/.config should be used. . GNU Stow . Some prominent results when googled for storing dotfiles are this Atlassian tutorial and using yadm. However, I found those harder to start using. . GNU Stow on the other hand is an easy to use symlink farm manager. As described in their website, it takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place. . This strategy works brilliantly for dotfiles. Borrowing explanation from Brandon Invergo’s article: . The procedure is simple. I created the ${HOME}/dotfiles directory and then inside it I made subdirectories for all the programs whose configurations I wanted to manage. Inside each of those directories, I moved in all the appropriate files, maintaining the directory structure of my home directory. So, if a file normally resides at the top level of your home directory, it would go into the top level of the program’s subdirectory. If a file normally goes in the default ${XDG_CONFIG_HOME}/${PKGNAME} location (${HOME}/.config/${PKGNAME}), then it would instead go in ${HOME}/dotfiles/${PKGNAME}/.config/${PKGNAME} and so on. . Install Stow . sudo apt stow # Ubuntu brew install stow # Homebrew Mac . Placing the files . Now, it might look complex at first. Let me explain with some examples. . .bashrc or .zshrc are present/needed in $HOME directory, so inside $HOME/dotfiles create a subdirectory with bashrc or zshrc and place the original .bashrc or .zshrc file appropriately inside their folder. GNU Stow understands that the dotfile, when symlinked, will create a symlink-copy in the $HOME directory. For future modifications, file in either locations can be edited. But for simplicity, use $HOME/dotfiles directory. | A complicated example would be a config file located deep inside subfolders: nvim’s or neovim’s init.vim or init.lua file. It is present in $HOME/.config/nvim/init.vim. For Stow to understand, it must be placed like this – $HOME/dotfiles/nvim/.config/nvim/init.vim | . For further reading, I recommend brilliantly written Jake Weisler’s post on GNU Stow. . Useful Stow commands . If correctly installed, then running the command stow --help should list options to use Stow. Most used commands are . stow &lt;packagename&gt; # activates symlink stow -n &lt;packagename&gt; # trial runs or simulates symlink generation. Effective for checking for errors stow -D &lt;packagename&gt; # delete stowed package stow -R &lt;packagename&gt; # restows package . Activating Stow . So if we have created three subdirectories inside dotfiles say zsh, git, nvim, then . stow bash git nvim . will activate their symlinks. . If returned to $HOME and $XDG_CONFIG_HOME and verified, then we will see, . .gitconfig -&gt; .dotfiles/git/.gitconfig .zshrc -&gt; .dotfiles/zsh/.zshrc nvim -&gt; ../.dotfiles/nvim/.config/nvim . The most awesome thing in all this is, the directory structure needs to be created only once. For future requirement, one simply clones the dotfiles directory and activates symlinks. . Storing files in Git . The dotfiles directory now becomes important to store in a remote location for safe keeping. Usually a git repository is the preferred method. For instructions on how to use git, look up various tutorials on Git in the internet. . In summary, I have written a short, albeit technical write up on GNU Stow, and its uses for storing dotfiles. Feel free to ask questions in the comments or via various means linked in the blog. .",
            "url": "https://dr563105.github.io/blog/manage-dotfiles-with-gnu-stow/",
            "relUrl": "/manage-dotfiles-with-gnu-stow/",
            "date": " • Jan 9, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Using json_normalize function",
            "content": "Javascript Object Notation(JSON) is a widely used format for storing and exchanging data. Coming from the relational database, it could be difficult to understand NoSQL databases that use JSON to store data and similarly REST API&#39;s response. JSON is also used in storing football event data. It allows easy addition of features in the future. . Though JSON format allows for easier exchange of data, for analysis, a tabular form would be appropriate. A JSON structure can be of two forms: a JSON object and list of JSON objects. Since our programming language of choice is Python, those structures can be somewhat called as a dictionary object or list of dicts. . . Importing pandas library . import pandas as pd . 1. Flattening a simple JSON . A dict . Let us consider a simple dictionary: 3 keys and their respective values. . viv = { &quot;player_id&quot; : 15623, &quot;player_name&quot; : &quot;Vivianne Miedema&quot;, &quot;jersey_number&quot; : 11} viv . {&#39;player_id&#39;: 15623, &#39;player_name&#39;: &#39;Vivianne Miedema&#39;, &#39;jersey_number&#39;: 11} . We use the json_normalize API to flatten a JSON dict. . df = pd.json_normalize(viv);df . player_id player_name jersey_number . 0 15623 | Vivianne Miedema | 11 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1 entries, 0 to 0 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 player_id 1 non-null int64 1 player_name 1 non-null object 2 jersey_number 1 non-null int64 dtypes: int64(2), object(1) memory usage: 152.0+ bytes . Side Note:If the data contains something that is not compatible with python, in this case a null variable, there are two choices: . Change null to None | Pass the data through json.loads function | . Change null to None . null = None viv1 = { &quot;player_id&quot; : 15623, &quot;player_name&quot; : &quot;Vivianne Miedema&quot;, &quot;jersey_number&quot; : 11, &quot;player_nickname&quot; : null} viv1 . {&#39;player_id&#39;: 15623, &#39;player_name&#39;: &#39;Vivianne Miedema&#39;, &#39;jersey_number&#39;: 11, &#39;player_nickname&#39;: None} . Make data as string and pass to json.loads . import json viv1 = &#39;{ &quot;player_id&quot; : 15623, &quot;player_name&quot; : &quot;Vivianne Miedema&quot;, &quot;jersey_number&quot; : 11, &quot;player_nickname&quot; : null}&#39; viv1 = json.loads(viv1) viv1 . {&#39;player_id&#39;: 15623, &#39;player_name&#39;: &#39;Vivianne Miedema&#39;, &#39;jersey_number&#39;: 11, &#39;player_nickname&#39;: None} . A list of dicts . player_list = [ { &quot;player_id&quot; : 15623, &quot;player_name&quot; : &quot;Vivianne Miedema&quot;, &quot;jersey_number&quot; : 11, &quot;player_nickname&quot; : null }, { &quot;player_id&quot; : 10658, &quot;player_name&quot; : &quot;Danielle van de Donk&quot;, &quot;jersey_number&quot; : 7, &quot;player_nickname&quot; : null } ] pd.json_normalize(player_list) . player_id player_name jersey_number player_nickname . 0 15623 | Vivianne Miedema | 11 | None | . 1 10658 | Danielle van de Donk | 7 | None | . We have the JSON list of dicts in a tabular form. All the keys become columns and their values as entries. . When we flattern a list with a key-value pair missing for an entry, instead of an error, NaN(not a number) is stored. . player_list = [ { &quot;player_id&quot; : 15623, &quot;player_name&quot; : &quot;Vivianne Miedema&quot;, &quot;jersey_number&quot; : 11, &quot;player_nickname&quot; : null }, { &quot;player_id&quot; : 10658, &quot;player_name&quot; : &quot;Danielle van de Donk&quot;} ] pd.json_normalize(player_list) . player_id player_name jersey_number player_nickname . 0 15623 | Vivianne Miedema | 11.0 | NaN | . 1 10658 | Danielle van de Donk | NaN | NaN | . Note: See how player_nickname when not specified also turns to NaN from None. . 2. Flattening a multi-level JSON . A simple dict . at_kick0ff = { &quot;id&quot;:&quot;d712fb93-c464-4621-98ba-f2bdcd5641db&quot;, &quot;timestamp&quot;:&quot;00:00:00.000&quot;, &quot;duration&quot;:0.0, &quot;lineup&quot;:{ &quot;player&quot;:{ &quot;id&quot;:15623, &quot;name&quot;:&quot;Vivianne Miedema&quot; }, &quot;position&quot;:{ &quot;id&quot;:23, &quot;name&quot;:&quot;Center Forward&quot; }, &quot;jersey_number&quot;:11 } } at_kick0ff . {&#39;id&#39;: &#39;d712fb93-c464-4621-98ba-f2bdcd5641db&#39;, &#39;timestamp&#39;: &#39;00:00:00.000&#39;, &#39;duration&#39;: 0.0, &#39;lineup&#39;: {&#39;player&#39;: {&#39;id&#39;: 15623, &#39;name&#39;: &#39;Vivianne Miedema&#39;}, &#39;position&#39;: {&#39;id&#39;: 23, &#39;name&#39;: &#39;Center Forward&#39;}, &#39;jersey_number&#39;: 11}} . pd.json_normalize(at_kick0ff) . id timestamp duration lineup.player.id lineup.player.name lineup.position.id lineup.position.name lineup.jersey_number . 0 d712fb93-c464-4621-98ba-f2bdcd5641db | 00:00:00.000 | 0.0 | 15623 | Vivianne Miedema | 23 | Center Forward | 11 | . You can see that lineup dictionary key&#39;s nested key-value pairs have been expanded into individual columns. If you feel that is unnecessary, we can restrict expansion by using max_level argument. With max_level=1, the flattening goes one level deeper. . pd.json_normalize(at_kick0ff, max_level=1) . id timestamp duration lineup.player lineup.position lineup.jersey_number . 0 d712fb93-c464-4621-98ba-f2bdcd5641db | 00:00:00.000 | 0.0 | {&#39;id&#39;: 15623, &#39;name&#39;: &#39;Vivianne Miedema&#39;} | {&#39;id&#39;: 23, &#39;name&#39;: &#39;Center Forward&#39;} | 11 | . A list of dicts . first_pass = [ { &quot;id&quot;:&quot;15758edb-58cd-49c4-a817-d2ef48ba3bcf&quot;, &quot;timestamp&quot;:&quot;00:00:00.504&quot;, &quot;type&quot;:{ &quot;id&quot;:30, &quot;name&quot;:&quot;Pass&quot; }, &quot;play_pattern&quot;:{ &quot;id&quot;:9, &quot;name&quot;:&quot;From Kick Off&quot; }, &quot;player&quot;:{ &quot;id&quot;:15623, &quot;name&quot;:&quot;Vivianne Miedema&quot; }, &quot;pass&quot;:{ &quot;recipient&quot;:{ &quot;id&quot;:10666, &quot;name&quot;:&quot;Dominique Johanna Anna Bloodworth&quot; }, &quot;length&quot;:25.455845, &quot;angle&quot;:-2.3561945, &quot;height&quot;:{ &quot;id&quot;:1, &quot;name&quot;:&quot;Ground Pass&quot; }, &quot;end_location&quot;:[ 42.0, 22.0 ] } }, { &quot;id&quot; : &quot;ab5674a4-e824-4143-9f6f-3f1645557413&quot;, &quot;timestamp&quot; : &quot;00:00:04.201&quot;, &quot;type&quot; : { &quot;id&quot; : 30, &quot;name&quot; : &quot;Pass&quot; }, &quot;play_pattern&quot; : { &quot;id&quot; : 9, &quot;name&quot; : &quot;From Kick Off&quot; }, &quot;player&quot; : { &quot;id&quot; : 10666, &quot;name&quot; : &quot;Dominique Johanna Anna Bloodworth&quot; }, &quot;location&quot; : [ 45.0, 29.0 ], &quot;duration&quot; : 1.795201, &quot;pass&quot; : { &quot;length&quot; : 51.62364, &quot;angle&quot; : 0.55038595, &quot;height&quot; : { &quot;id&quot; : 3, &quot;name&quot; : &quot;High Pass&quot; }, &quot;end_location&quot; : [ 89.0, 56.0 ] } } ] pd.json_normalize(first_pass) . id timestamp type.id type.name play_pattern.id play_pattern.name player.id player.name pass.recipient.id pass.recipient.name pass.length pass.angle pass.height.id pass.height.name pass.end_location location duration . 0 15758edb-58cd-49c4-a817-d2ef48ba3bcf | 00:00:00.504 | 30 | Pass | 9 | From Kick Off | 15623 | Vivianne Miedema | 10666.0 | Dominique Johanna Anna Bloodworth | 25.455845 | -2.356194 | 1 | Ground Pass | [42.0, 22.0] | NaN | NaN | . 1 ab5674a4-e824-4143-9f6f-3f1645557413 | 00:00:04.201 | 30 | Pass | 9 | From Kick Off | 10666 | Dominique Johanna Anna Bloodworth | NaN | NaN | 51.623640 | 0.550386 | 3 | High Pass | [89.0, 56.0] | [45.0, 29.0] | 1.795201 | . Limiting the levels... . pd.json_normalize(first_pass, max_level=0) . id timestamp type play_pattern player pass location duration . 0 15758edb-58cd-49c4-a817-d2ef48ba3bcf | 00:00:00.504 | {&#39;id&#39;: 30, &#39;name&#39;: &#39;Pass&#39;} | {&#39;id&#39;: 9, &#39;name&#39;: &#39;From Kick Off&#39;} | {&#39;id&#39;: 15623, &#39;name&#39;: &#39;Vivianne Miedema&#39;} | {&#39;recipient&#39;: {&#39;id&#39;: 10666, &#39;name&#39;: &#39;Dominique... | NaN | NaN | . 1 ab5674a4-e824-4143-9f6f-3f1645557413 | 00:00:04.201 | {&#39;id&#39;: 30, &#39;name&#39;: &#39;Pass&#39;} | {&#39;id&#39;: 9, &#39;name&#39;: &#39;From Kick Off&#39;} | {&#39;id&#39;: 10666, &#39;name&#39;: &#39;Dominique Johanna Anna ... | {&#39;length&#39;: 51.62364, &#39;angle&#39;: 0.55038595, &#39;hei... | [45.0, 29.0] | 1.795201 | . 3. Flattening a JSON nested list . A simple dict . For this case, let us consider a simpler example than of football event data. The key info has list of dictionaries inside its structure. We call it nested dict. . awfc = { &#39;team&#39;: &#39;AWFC&#39;, &#39;location&#39;: &#39;London&#39;, &#39;ranking&#39;: 1, &#39;info&#39;: { &#39;manager&#39;: &#39;Joe&#39;, &#39;contacts&#39;: { &#39;email&#39;: { &#39;coaching&#39;: &#39;joe@afc.com&#39;, &#39;general&#39;: &#39;info@afc.com&#39; }, &#39;tel&#39;: &#39;123456789&#39;, } }, &#39;players&#39;: [ { &#39;name&#39;: &#39;Viv&#39; }, { &#39;name&#39;: &#39;DvD&#39; }, { &#39;name&#39;: &#39;Kim&#39; } ], };awfc . {&#39;team&#39;: &#39;AWFC&#39;, &#39;location&#39;: &#39;London&#39;, &#39;ranking&#39;: 1, &#39;info&#39;: {&#39;manager&#39;: &#39;Joe&#39;, &#39;contacts&#39;: {&#39;email&#39;: {&#39;coaching&#39;: &#39;joe@afc.com&#39;, &#39;general&#39;: &#39;info@afc.com&#39;}, &#39;tel&#39;: &#39;123456789&#39;}}, &#39;players&#39;: [{&#39;name&#39;: &#39;Viv&#39;}, {&#39;name&#39;: &#39;DvD&#39;}, {&#39;name&#39;: &#39;Kim&#39;}]} . The players column has a list of dicts. So, we can flatten that column using record_path argument. . pd.json_normalize(awfc, record_path=[&#39;players&#39;]) . name . 0 Viv | . 1 DvD | . 2 Kim | . But, making a separate table with no reference id has no meaning. To prevent that we can append revelant columns to the new table using meta argument. Here we want their team and Telephone number. The tel key lies within info-&gt;contacts-&gt;tel. So, we need provide that path like so [&#39;info&#39;, &#39;contacts&#39;, &#39;tel&#39;]. . pd.json_normalize(awfc, record_path=[&#39;players&#39;], meta=[&#39;team&#39;,[&#39;info&#39;, &#39;contacts&#39;, &#39;tel&#39;]]) . name team info.contacts.tel . 0 Viv | AWFC | 123456789 | . 1 DvD | AWFC | 123456789 | . 2 Kim | AWFC | 123456789 | . The order in which those paths are mentioned, the order in which those columns are appended. . pd.json_normalize(awfc, record_path=[&#39;players&#39;], meta=[&#39;team&#39;,[&#39;info&#39;, &#39;contacts&#39;, &#39;tel&#39;],[&#39;info&#39;, &#39;manager&#39;]]) . name team info.contacts.tel info.manager . 0 Viv | AWFC | 123456789 | Joe | . 1 DvD | AWFC | 123456789 | Joe | . 2 Kim | AWFC | 123456789 | Joe | . A list of dicts . json_list = [ { &#39;team&#39;: &#39;arsenal&#39;, &#39;colour&#39;: &#39;red-white&#39;, &#39;info&#39;: { &#39;staff&#39;: { &#39;physio&#39;: &#39;xxxx&#39;, &#39;doctor&#39;: &#39;yyyy&#39; } }, &#39;players&#39;: [ { &#39;name&#39;: &#39;Viv&#39;, &#39;sex&#39;: &#39;F&#39;, &#39;stats&#39;: { &#39;goals&#39;: 101, &#39;assists&#39;: 40 } }, { &#39;name&#39;: &#39;Beth&#39;, &#39;sex&#39;: &#39;F&#39;, &#39;stats&#39;: { &#39;goals&#39;: 60, &#39;assists&#39;: 25 } }, ] }, { &#39;team&#39;: &#39;city&#39;, &#39;colour&#39;: &#39;blue&#39;, &#39;info&#39;: { &#39;staff&#39;: { &#39;physio&#39;: &#39;aaaa&#39;, &#39;doctor&#39;: &#39;bbbb&#39; } }, &#39;players&#39;: [ { &#39;name&#39;: &#39;Steph&#39;, &#39;sex&#39;: &#39;F&#39; }, { &#39;name&#39;: &#39;Lucy&#39;, &#39;sex&#39;: &#39;F&#39; }, ] }, ] pd.json_normalize(json_list) . team colour players info.staff.physio info.staff.doctor . 0 arsenal | red-white | [{&#39;name&#39;: &#39;Viv&#39;, &#39;sex&#39;: &#39;F&#39;, &#39;stats&#39;: {&#39;goals&#39;... | xxxx | yyyy | . 1 city | blue | [{&#39;name&#39;: &#39;Steph&#39;, &#39;sex&#39;: &#39;F&#39;}, {&#39;name&#39;: &#39;Lucy... | aaaa | bbbb | . pd.json_normalize(json_list, record_path =[&#39;players&#39;]) . name sex stats.goals stats.assists . 0 Viv | F | 101.0 | 40.0 | . 1 Beth | F | 60.0 | 25.0 | . 2 Steph | F | NaN | NaN | . 3 Lucy | F | NaN | NaN | . How about we now append the players&#39; team, colour, and their physio. . pd.json_normalize( json_list, record_path =[&#39;players&#39;], meta=[&#39;team&#39;, &#39;colour&#39;, [&#39;info&#39;, &#39;staff&#39;, &#39;physio&#39;]] ) . name sex stats.goals stats.assists team colour info.staff.physio . 0 Viv | F | 101.0 | 40.0 | arsenal | red-white | xxxx | . 1 Beth | F | 60.0 | 25.0 | arsenal | red-white | xxxx | . 2 Steph | F | NaN | NaN | city | blue | aaaa | . 3 Lucy | F | NaN | NaN | city | blue | aaaa | . 4. Ignoring key errors . json_list = [ { &#39;team&#39;: &#39;arsenal&#39;, &#39;colour&#39;: &#39;red-white&#39;, &#39;info&#39;: { &#39;staff&#39;: { &#39;physio&#39;: &#39;xxxx&#39;, &#39;doctor&#39;: &#39;yyyy&#39; } }, &#39;players&#39;: [ { &#39;name&#39;: &#39;Viv&#39;, &#39;sex&#39;: &#39;F&#39;, &#39;stats&#39;: { &#39;goals&#39;: 101, &#39;assists&#39;: 40 } }, { &#39;name&#39;: &#39;Beth&#39;, &#39;sex&#39;: &#39;F&#39;, &#39;stats&#39;: { &#39;goals&#39;: 60, &#39;assists&#39;: 25 } }, ] }, { &#39;team&#39;: &#39;city&#39;, &#39;colour&#39;: &#39;blue&#39;, &#39;info&#39;: { &#39;staff&#39;: { &#39;doctor&#39;: &#39;bbbb&#39; } }, &#39;players&#39;: [ { &#39;name&#39;: &#39;Steph&#39;, &#39;sex&#39;: &#39;F&#39; }, { &#39;name&#39;: &#39;Lucy&#39;, &#39;sex&#39;: &#39;F&#39; }, ] }, ] . Notice that the key physio is missing from the entry team=city. What happens if we try to access physio key inside meta? . pd.json_normalize( json_list, record_path =[&#39;players&#39;], meta=[&#39;team&#39;, &#39;colour&#39;, [&#39;info&#39;, &#39;staff&#39;, &#39;physio&#39;]], ) . KeyError Traceback (most recent call last) /opt/homebrew/Caskroom/miniforge/base/envs/jupnet/lib/python3.8/site-packages/pandas/io/json/_normalize.py in _recursive_extract(data, path, seen_meta, level) 491 try: --&gt; 492 meta_val = _pull_field(obj, val[level:]) 493 except KeyError as e: /opt/homebrew/Caskroom/miniforge/base/envs/jupnet/lib/python3.8/site-packages/pandas/io/json/_normalize.py in _pull_field(js, spec) 387 for field in spec: --&gt; 388 result = result[field] 389 else: KeyError: &#39;physio&#39; The above exception was the direct cause of the following exception: KeyError Traceback (most recent call last) /var/folders/27/bh1fc62j2yq48m_4vk_tgq_80000gp/T/ipykernel_8402/2719010332.py in &lt;module&gt; -&gt; 1 pd.json_normalize( 2 json_list, 3 record_path =[&#39;players&#39;], 4 meta=[&#39;team&#39;, &#39;colour&#39;, [&#39;info&#39;, &#39;staff&#39;, &#39;physio&#39;]], 5 ) /opt/homebrew/Caskroom/miniforge/base/envs/jupnet/lib/python3.8/site-packages/pandas/io/json/_normalize.py in _json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level) 502 records.extend(recs) 503 --&gt; 504 _recursive_extract(data, record_path, {}, level=0) 505 506 result = DataFrame(records) /opt/homebrew/Caskroom/miniforge/base/envs/jupnet/lib/python3.8/site-packages/pandas/io/json/_normalize.py in _recursive_extract(data, path, seen_meta, level) 495 meta_val = np.nan 496 else: --&gt; 497 raise KeyError( 498 &#34;Try running with errors=&#39;ignore&#39; as key &#34; 499 f&#34;{e} is not always present&#34; KeyError: &#34;Try running with errors=&#39;ignore&#39; as key &#39;physio&#39; is not always present&#34; . How come stats.goals and stats.assists didn&#39;t generate an error but that above does? Because, the meta argument expects values to be present for listed keys in meta by default. We can ignore those errors(as suggested) using errors=&#39;ignore&#39; . pd.json_normalize( json_list, record_path =[&#39;players&#39;], meta=[&#39;team&#39;, &#39;colour&#39;, [&#39;info&#39;, &#39;staff&#39;, &#39;physio&#39;]], errors=&#39;ignore&#39; ) . name sex stats.goals stats.assists team colour info.staff.physio . 0 Viv | F | 101.0 | 40.0 | arsenal | red-white | xxxx | . 1 Beth | F | 60.0 | 25.0 | arsenal | red-white | xxxx | . 2 Steph | F | NaN | NaN | city | blue | NaN | . 3 Lucy | F | NaN | NaN | city | blue | NaN | . 5. Custom separator sep . We notice that by default pandas uses . to indicate the direction of the path. We can change that using the sep argument. . Tip:Usually an underscore is used instead of . . json_list = [ { &#39;team&#39;: &#39;arsenal&#39;, &#39;colour&#39;: &#39;red-white&#39;, &#39;info&#39;: { &#39;staff&#39;: { &#39;physio&#39;: &#39;xxxx&#39;, &#39;doctor&#39;: &#39;yyyy&#39; } }, &#39;players&#39;: [ { &#39;name&#39;: &#39;Viv&#39;, &#39;sex&#39;: &#39;F&#39;, &#39;stats&#39;: { &#39;goals&#39;: 101, &#39;assists&#39;: 40 } }, { &#39;name&#39;: &#39;Beth&#39;, &#39;sex&#39;: &#39;F&#39;, &#39;stats&#39;: { &#39;goals&#39;: 60, &#39;assists&#39;: 25 } }, ] }, { &#39;team&#39;: &#39;city&#39;, &#39;colour&#39;: &#39;blue&#39;, &#39;info&#39;: { &#39;staff&#39;: { &#39;physio&#39;: &#39;aaaa&#39;, &#39;doctor&#39;: &#39;bbbb&#39; } }, &#39;players&#39;: [ { &#39;name&#39;: &#39;Steph&#39;, &#39;sex&#39;: &#39;F&#39; }, { &#39;name&#39;: &#39;Lucy&#39;, &#39;sex&#39;: &#39;F&#39; }, ] }, ] . pd.json_normalize( json_list, record_path =[&#39;players&#39;], meta=[&#39;team&#39;, &#39;colour&#39;, [&#39;info&#39;, &#39;staff&#39;, &#39;physio&#39;]], sep=&#39;-&gt;&#39; ) . name sex stats-&gt;goals stats-&gt;assists team colour info-&gt;staff-&gt;physio . 0 Viv | F | 101.0 | 40.0 | arsenal | red-white | xxxx | . 1 Beth | F | 60.0 | 25.0 | arsenal | red-white | xxxx | . 2 Steph | F | NaN | NaN | city | blue | aaaa | . 3 Lucy | F | NaN | NaN | city | blue | aaaa | . 6. Adding context to record and meta data using record_prefix and meta_prefix . pd.json_normalize( json_list, record_path=[&#39;players&#39;], meta=[&#39;team&#39;, &#39;colour&#39;, [&#39;info&#39;, &#39;staff&#39;, &#39;physio&#39;]], meta_prefix=&#39;meta-&#39;, record_prefix=&#39;player-&#39;, sep=&#39;-&gt;&#39; ) . player-name player-sex player-stats-&gt;goals player-stats-&gt;assists meta-team meta-colour meta-info-&gt;staff-&gt;physio . 0 Viv | F | 101.0 | 40.0 | arsenal | red-white | xxxx | . 1 Beth | F | 60.0 | 25.0 | arsenal | red-white | xxxx | . 2 Steph | F | NaN | NaN | city | blue | aaaa | . 3 Lucy | F | NaN | NaN | city | blue | aaaa | . 7. Working with a local file . In most scenarios, we won&#39;t be making new JSON object ourselves instead use JSON formatted files. We make use python&#39;s json module and read the file, then use pandas&#39; json_normalize to flatten it into a dataframe. . import json # load data using Python JSON module with open(&#39;my_attachment/movies.json&#39;) as f: data = json.load(f) # Normalizing data pd.json_normalize(data) . Title US Gross Worldwide Gross US DVD Sales Production Budget Release Date MPAA Rating Running Time min Distributor Source Major Genre Creative Type Director Rotten Tomatoes Rating IMDB Rating IMDB Votes . 0 The Land Girls | 146083 | 146083 | NaN | 8000000 | Jun 12 1998 | R | NaN | Gramercy | None | None | None | None | NaN | 6.1 | 1071.0 | . 1 First Love, Last Rites | 10876 | 10876 | NaN | 300000 | Aug 07 1998 | R | NaN | Strand | None | Drama | None | None | NaN | 6.9 | 207.0 | . 2 I Married a Strange Person | 203134 | 203134 | NaN | 250000 | Aug 28 1998 | None | NaN | Lionsgate | None | Comedy | None | None | NaN | 6.8 | 865.0 | . 3 Four Rooms | 4301000 | 4301000 | NaN | 4000000 | Dec 25 1995 | R | NaN | Miramax | Original Screenplay | Comedy | Contemporary Fiction | Robert Rodriguez | 14.0 | 6.4 | 34328.0 | . 4 The Four Seasons | 42488161 | 42488161 | NaN | 6500000 | May 22 1981 | None | NaN | Universal | Original Screenplay | Comedy | Contemporary Fiction | Alan Alda | 71.0 | 7.0 | 1814.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 63 Big Things | 0 | 0 | NaN | 50000 | Dec 31 2009 | None | NaN | None | None | None | None | None | NaN | NaN | NaN | . 64 Bogus | 4357406 | 4357406 | NaN | 32000000 | Sep 06 1996 | PG | NaN | Warner Bros. | Original Screenplay | Comedy | Fantasy | Norman Jewison | 40.0 | 4.8 | 2742.0 | . 65 Beverly Hills Cop | 234760478 | 316300000 | NaN | 15000000 | Dec 05 1984 | None | NaN | Paramount Pictures | Original Screenplay | Action | Contemporary Fiction | Martin Brest | 83.0 | 7.3 | 45065.0 | . 66 Beverly Hills Cop II | 153665036 | 276665036 | NaN | 20000000 | May 20 1987 | R | NaN | Paramount Pictures | Original Screenplay | Action | Contemporary Fiction | Tony Scott | 46.0 | 6.1 | 29712.0 | . 67 Beverly Hills Cop III | 42586861 | 119180938 | NaN | 50000000 | May 25 1994 | R | NaN | Paramount Pictures | Original Screenplay | Action | Contemporary Fiction | John Landis | 10.0 | 5.0 | 21199.0 | . 68 rows × 16 columns . 8. Working with URL . Reading a JSON file from an url needs an extra module in requests as any data from the Internet carries overheads that are necessary for efficient exchange of information(REST API). So, in order to read the file contents, we call upon requests&#39; text attribute which fetches the contents of the file. . Here, we use json.loads and not json.load as loads function expects contents(string) rather than a file pointer. If looked closely into the json module, the load calls loads using read() on the file. . import requests URL = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; data = json.loads(requests.get(URL).text) pd.json_normalize(data) . Name Miles_per_Gallon Cylinders Displacement Horsepower Weight_in_lbs Acceleration Year Origin . 0 chevrolet chevelle malibu | 18.0 | 8 | 307.0 | 130.0 | 3504 | 12.0 | 1970-01-01 | USA | . 1 buick skylark 320 | 15.0 | 8 | 350.0 | 165.0 | 3693 | 11.5 | 1970-01-01 | USA | . 2 plymouth satellite | 18.0 | 8 | 318.0 | 150.0 | 3436 | 11.0 | 1970-01-01 | USA | . 3 amc rebel sst | 16.0 | 8 | 304.0 | 150.0 | 3433 | 12.0 | 1970-01-01 | USA | . 4 ford torino | 17.0 | 8 | 302.0 | 140.0 | 3449 | 10.5 | 1970-01-01 | USA | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 401 ford mustang gl | 27.0 | 4 | 140.0 | 86.0 | 2790 | 15.6 | 1982-01-01 | USA | . 402 vw pickup | 44.0 | 4 | 97.0 | 52.0 | 2130 | 24.6 | 1982-01-01 | Europe | . 403 dodge rampage | 32.0 | 4 | 135.0 | 84.0 | 2295 | 11.6 | 1982-01-01 | USA | . 404 ford ranger | 28.0 | 4 | 120.0 | 79.0 | 2625 | 18.6 | 1982-01-01 | USA | . 405 chevy s-10 | 31.0 | 4 | 119.0 | 82.0 | 2720 | 19.4 | 1982-01-01 | USA | . 406 rows × 9 columns . Conclusion . We saw the use of json_normalize function in pandas library. It helps take a JSON data, flatten it, and make it as a dataframe for easier analysis. . References . B. Chen, https://towardsdatascience.com/all-pandas-json-normalize-you-should-know-for-flattening-json-13eae1dfb7dd | Pandas documentation, https://pandas.pydata.org/pandas-docs/version/1.2.0/reference/api/pandas.json_normalize.html |",
            "url": "https://dr563105.github.io/blog/json_normalize_fn/",
            "relUrl": "/json_normalize_fn/",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Setting up Kaggle on Linux/Mac",
            "content": "Most of latest data science innovations happen at Kaggle. Kaggle hosts, in addtion to competitions, a large collection of datasets from various fields. The easiest way to interact with Kaggle is through its public API via command-line tool(CLI). Setting it up outside of Kaggle kernels is one of first tasks. In this post, I will guide you through that process. . Pre-requisite: Python3(&gt;3.6) and latest pip installed. . Installation . pip install --user kaggle . Tip: Install kaggle package inside your conda ML development environment rather than outside of it or in base env. . Warning: Don’t do sudo pip install kaggle as it would require admin privileges for every run. . Download API token . Create/login into your kaggle account. | From the site header, click on your user profile picture and select Account. You will be land on your profile with account tab active. | Scroll down to API section. Click Create New API Token. A json file will be downloaded your default download directory. | Move .json file to the correct location . Move it to .kaggle in the home directory. Create if absent. cd mkdir ~/.kaggle mv &lt;location&gt;/kaggle.json ~/.kaggle/kaggle.json . | For your security, ensure that other users of your computer do not have read access to your credentials. On Unix-based systems you can do this with the following command: chmod 600 ~/.kaggle/kaggle.json . | Restart the terminal and navigate to the env where kaggle package is installed if necessary. | Check if it is properly installed . Run: $python &gt;&gt;&gt;import kaggle . Importing kaggle shouldn’t return an error. If there is error, check whether you’re in the right env where kaggle is installed. . | If no error, exit the shell and type the following command in the terminal. . kaggle competitions list . If installed properly, the command will list all the entered competitions. . If not, the binary path may be incorrect. Usually it is installed in ~/.local/bin Try using ~/.local/bin/kaggle competitions list . | If the above command works, export that binary path to the shell environment(bashrc) so that you might use just kaggle next time. | API usage . It is time to use the Kaggle API. For example, to see what dataset command offers, in the CLI enter . kaggle dataset --help . Tip: Remember to comply with competition’s terms and conditions before downloading the dataset. You will get an error forbidden if you try to download before agreeing. . For more info on the API, Kaggle’s github page is an excellent resource. .",
            "url": "https://dr563105.github.io/blog/kaggle-setup/",
            "relUrl": "/kaggle-setup/",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Setting up AWS EC2 instance for ML",
            "content": "Amazon Web Services(AWS) offer great many services to suit user needs. For ML/DL all one needs is a computer with access to a Graphical Processing Unit(GPU). Since AWS offers a virtual PC with GPU at a reasonable price, we will use their service. This blog post will use an AWS EC2 instance to make it ready for running ML tasks. . Pre-requisites: This guide expects you to be familiar with linux(Ubuntu) environment and activated your AWS account. . Fast.ai . We will use Fastai library to make our ML models. Fastai is a open source software library for deep learning. Developing ML models using fastai makes it much easier than just Pytorch. . Note: This post is similar to the fastai’s guide for AWS setup but with added commentary and updated final instructions. . AWS EC2 . AWS EC2 is a service that provides a server PC for our use. It comes with a template called amazon machine image(AMI) that makes it so easy to launch a virtual cloud server in less than two minutes. . For our ML purpose, we will use g4dn.xlarge EC2 on-demand instance. This particular instance has a 16GB Nvidia T4 GPU. For more info on instances, check out this page at AWS. . Note: Servers in regions(also known as availability zones(AZ)) such as North Virginia(N. Virginia), Ohio offer the chepeast prices for g4dn instances starting at $0.526 per hour. Check out the pricing here. . Requesting service limit increase . To avoid misuse of GPU, amazon restricts its usage. However, we can request for service limit increase with a support ticket. It is important to remember, service limit needs to be increased for each region. The number of vCPUs an instance has, is a good measure of its capacity. In this case, request 16vCPUs(upto g4dn.4xlarge) in the support ticket. Follow this step to get access to a GPU. . Tip: Usually a description of ‘to use for ML/DL course and training models’ would be an acceptable reason for approval. . Import key pair to AWS EC2 region . Use this amazon guide to import generate and import rsa key pair to an AWS EC2 region or AZ. It is important to store the private key in a secure directory. . Launch instance . Use this fastai step to launch an g4dn.xlarge instance. . Connect to instance . Using the public IP of the instance and rsa private key we can login into our instance. . Login using . ssh -i &lt;path-to-pem-file&gt; ubuntu@&lt;ip-address&gt; . The username is ubuntu for ubuntu EC2 instance, ec2-user for amazon AMI, and so on. We will be using ubuntu ec2 g4dn instance. . Example: . ssh -i ~/.ssh/myprivatekey.pem ubuntu@&lt;ip&gt; . You may be prompted about trusting this address, to which you should reply ‘yes’. . Solution to possible shell environment problems: In some cases if the host terminal is using a different XTERM environment such as xterm-kitty(echo $TERM), that environment is reflected in the remote EC2 instance. If that is the case, it best to use this command: . TERM=&#39;xterm-256color&#39; ssh -i &lt;path-to-pem-file&gt; ubuntu@&lt;ip-address&gt; . Also a possibility would be to add the TERM env to bashrc file. . echo &#39;export TERM=xterm-256color&#39; &gt;&gt; ~/.bashrc source ~/.bashrc . Why the need to do to set the TERM env? Because, remote host will have the basic env set and the keystrokes such as previously run command(up arrow) would return a scrambled output on the screen. . Setup Ubuntu server . First, to do basic ubuntu configurations, such as updating packages, and turning on auto-updates, execute: . sudo apt update &amp;&amp; sudo apt -y install git git clone https://github.com/dr563105/fastsetup.git cd fastsetup sudo ./ubuntu-initial.sh . The setup shell script will create a new user inside the cloud server, installs/updates the necessary packages and libraries, installs firewalls, and sets up ssh. . Note: This new user creation is not to be confused with IAM user created by AWS root user. . Reboot when prompted. Wait a couple of minutes for reboot, then ssh back in. . To reconnect using ssh, add an additional -L flag which will allow you to open up ports to connect to Jupyter Notebook once it’s installed: . ssh -i &lt;path-to-pem-file&gt; -L localhost:8888:localhost:8888 ubuntu@&lt;ip-address&gt; . (Optional): Change default shell to ZSH with Oh My ZSH: . sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot; &quot;&quot; --unattended . Change default SHELL to ZSH using . sudo chsh $USER -s /bin/zsh . When prompted for password, use the password entered at the user creation step during the setup process. . Logout and login again to activate ZSH shell. It is possible to switch without disconnecting but I find this step simpler to other solutions. . Setup Conda . Conda is an open source package management system to setup environments irrespective of the OS. One of the major advantages of conda is that it easily installs all the dependencies for a package. Anaconda provides a conda installer with thousands of packages. However, for our case, we will use Miniconda, a minimal installer of conda. . cd fastsetup ./setup-conda.sh . Note: Deviates from fastai’s fastsetup setup-conda.shscript in creating a .condarc file. My experiments have shown this file to be troublesome. . If you’ve worked with conda installs before, you’ll know that it is too slow. Mamba is a reimplementation of the conda package manager in C++. So we install that next. . source ~/.bashrc (or source ~/.zshrc) conda install mamba -n base -c fastchan . We use fastchan as the channel source. fastchan is an anaconda pacakage from fastai team. A detailed post on fastchan from weights and biases. . Don’t mix conda-forge with fastai channels. Stick to either fastchan or anaconda. . Install Nvidia . It is much faster to train ML models in a GPU. Currently only Nvidia GPUs are supported. Since our instance has a GPU, we need to install its drivers and activate it. . Use this command to list available drivers: ubuntu-drivers devices . Tip: Choose the “recommended” option, plus the -server suffix. . When you install “470” might be a different number, based on ubuntu-drivers output above . sudo apt-fast install -y nvidia-driver-470-server sudo modprobe nvidia nvidia-smi . When prompted for a password, enter the password you entered while you ran the ubuntu setup script. . Note: The command installs cuda 11.4. This is not to be confused with cudatoolkit=11.1 which is needed for pytorch=1.9. Also each pytorch conda install comes with its own cuDNN runtime. So installing cuDNN separately is not needed. cuda 11.4 comes to play if pytorch is built from source. . Create conda environment . It is good practice to install the necessary packages in a new conda environment. Installing everything in the conda base environment is not advisable. . conda create -n &lt;envname&gt; -y conda create -n mlenv -y conda activate mlenv . Install fastbook with all its dependencies including CUDA enabled pytorch libraries . Now you’re ready to install all needed packages for the fast.ai course: Make there is enough space to install(df -h). You need about 15GB of space. . mamba install fastbook python=3.8 -c fastai -c fastchan -y conda install pytorch torchaudio torchvision python=3.8 cudatoolkit=11.1 -c fastchan -y . Fastbook is a fastai’s book on using fastai for ML development. As a python package, it installs all the dependent packages. To see what it install remove -y from the command. . Usually one command(mamba install fastbook) was enough previously to install everything. However, there seems to be package conflicts when executed now. Hence, the additional conda install step which upgrades pytorch, torchvision to the latest GPU version. . Tip: For dry run, use ‘-d’ argument in the mamba install command. . Note: Installing fastbook doesn’t mean you must use fastai. It just installs everything needed for ML tasks. If you’ve noticed carefully while installing, it is Pytorch, Cuda packages that require huge memory space. Other packages come just under 2 GB. . Sanity checks: . These checks are to verifying if indeed CUDA enabled pytorch is installed correctly. Enter these commands inside a python shell. . $which python /home/ubuntu/miniconda3/envs/mlenv/bin/python $python --version Python 3.8.5 $python &gt;&gt;&gt;import torch &gt;&gt;&gt;torch.version.cuda &#39;11.1&#39; &gt;&gt;&gt;torch.cuda.is_available() True &gt;&gt;&gt; torch.cuda.device_count() 1 &gt;&gt;&gt;torch.cuda.current_device() 0 &gt;&gt;&gt;torch.cuda.get_device_name(0) &#39;Tesla T4&#39; . If the commands return the same results, then we’re ready for ML development. . Run jupyter notebook . To download the notebooks, run: . cd git clone https://github.com/fastai/fastbook cd fastbook jupyter notebook . Click on the localhost url that is displayed. It will open iPython notebook in your default browser. Alternatively, that link can be copied and opened in any browser of choice. . There are other Jupyter notebook guides that require to add a password for access. So far I’ve not needed those steps for my tasks. . (Optional) Email Setup: . To set up email: . sudo ./opensmtpd-install.sh . To test email, create a text file msg containing a message to send, then send it with: . cat msg | mail -r &quot;x@$(hostname -d)&quot; -s &#39;subject&#39; EMAIL_ADDR . Replace EMAIL_ADDR with an address to send to. You can get a useful testing address from mail-tester. . Stopping the instance . After you have finished your work, unless a training is going on, it is critical to stop the instance, if you wish to save your AWS bill. . Use either EC2 dashboard to stop the instance(instance-&gt;actions-&gt;instance state-&gt;stop) or use this command in the terminal . sudo shutdown -h now . Note: Pressing terminate will remove the instance completely and your work will be lost forever. . Wrapping up . There it is: an instance that is ready for ML development. If you have questions or feedback, reach me through the comments or via twitter. .",
            "url": "https://dr563105.github.io/blog/aws-setup/",
            "relUrl": "/aws-setup/",
            "date": " • Sep 17, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Deepak",
          "content": "Hello welcome to my blog! . Before I introduce myself, please tell me how did you stumble into my blog? How do you find the posts? Are they informative or hard to understand or boring/too long or full of mistakes? Your feedback is important for me. Contact me here or via twitter. Love to interact with you. . So, since you’re curious about me, I’d happily tell a few things describing myself. . My name is Deepak Ramani. I’m an aspiring Machine Learning Engineer/Data Analyst. I hold a Masters degree from RWTH Aachen University in Communications engineering. My thesis was based on autonomous driving involving ROS. If you’re interested you can read about it here. . Prior to going to Germany, I worked at Infosys Technologies for about two years. This was a valuable experience for me to know how corporate, agile methodologies work. . I strive to improve myself on things I’m working on. I look for perfection in any work I do. I love feedback be it positive or negative. Perhaps because of my upbringing, I enjoy working in a team that share my ideologies and mentality to achieve goals with perfection. . I’m open to full-time employment either remote or on-site. I’d be interested in Data Analyst, Data Engineer, Data Scientist, ML engineer roles. . If you have interest in seeing my resume, please contact me. . Und ich kann gerne auf Deutsch mit dir reden. Bitte schreib mir! .",
          "url": "https://dr563105.github.io/blog/about-me/",
          "relUrl": "/about-me/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dr563105.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}